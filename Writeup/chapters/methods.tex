%!TEX root = ../thesis.tex
% \begin{savequote}[75mm]
% Nulla facilisi. In vel sem. Morbi id urna in diam dignissim feugiat. Proin molestie tortor eu velit. Aliquam erat volutpat. Nullam ultrices, diam tempus vulputate egestas, eros pede varius leo.
% \qauthor{Quoteauthor Lastname}
% \end{savequote}

\chapter{Methods}
\label{Methods}

\newthought{Throughout this project},  we work with the HeartSteps v1 Data, hereupon abbreviated HSv1.  Features have been created from the measurements through domain science, through which we assume that the Bandit algorithms use linear models for the purposes of the project.

The contextual bandit algorithms used are stochastic, meaning for every user $n$, day $t$, and decision point $t$, the algorithm generates a probability $\pi_{n,t,d}$ of action.  Thus, whethere an action is taken is generated by

\begin{align*}
A_{n,t,d} \sim \operatorname{Bern}(\pi_{n,t,d}).
\end{align*}

The main metric of performance on a user we use is Mean User Expected Reward, or $MUER$. This is computed in equation \ref{MUER} using the reward $R^{(a)}_{n,t,d}$ of either action $a = 0$ or $a = 1$ under our generative model:
\begin{align}
\label{MUER}
MUER(\mathcal{S}, \Theta, \varepsilon,n) &= \frac{1}{TD} \sum_{\substack{t=1,\ldots,T \\ d=1,\ldots,D}} \pi_{n,t,d} R^{(1)}_{n,t,d} + (1-\pi_{n,t,d}) R^{(0)}_{n,t,d}, \\
\label{GenReward}
R^{(a)}_{n,t,d} &= \begin{bmatrix} f_1(S_{n,t,d}) \\
a \cdot f_2(S_{n,t,d})
\end{bmatrix}^T \Theta + \epsilon_{n,t,d},
\end{align}

Note that we add in $\epsilon_{n,t,d}$ to better account for misspecification in our `true' generative model.


For each simulation of $N$ users, we can now compute the mean and standard deviation of $MUER$ as our main performance metrics. \\

\section{Methodology Overview}

For simulations, our overall methodology is the following.  We first designate a variant of the Bandit algorithm, as well as a `true' generative model, using $f_1,f_2$, to form simulated rewards from the context and given action.  Next, we split HSv1 user data into $K$-fold cross validation sets.  For each split, we perform the below process:

\begin{enumerate}
	\item Use the training and testing data to generate training simulated users and testing simulated users, with $N = 500$ users in simulation.
	\item Use training simulated users to tune parameters of the given Bandit algorithm based on the mean of $MUER$ across all users and decision points, contingent on quality metrics.
	\item Run simulated users from test data using tuned parameters, observing quality metrics and impact of each tuning parameter on mean and std $MUER$.
\end{enumerate}

Each part is described in more detail below.


  \begin{table}
 \caption{Notations}
 \label{Notation Table}
 \centering \begin{tabular*}
{0.987\textwidth}
{|p{0.15\textwidth}|p{0.25\textwidth}|p{0.5\textwidth}|}
\toprule
Term & Name & Description \\
\midrule
$\mathcal{S},S_{n,t,d}$ & Context & Set of 7 features     \\
$p_1$ & Baseline features dimension & Full set consists of 7 features with 1 bias term, Small set consists of 2 with 1 bias term \\
$p_2$ & Interaction features dimension & Full set consists of 3 features with 1 bias term, Small set consists of 2 with 1 bias term \\
$\mathcal{A}, a_{n,t,d}$ & Actions &  Binary -- $1$: active message sent, $0$: no active message sent   \\
$\mathcal{R}, R_{n,t,d}$ & Reward &  Log-transformed step count in $30$ minutes following decision point  \\
$f_1 : \mathcal{S} \to \mathbb{R}^{p_1}$ & Baseline feature mapping & Maps context to baseline features \\
$f_2 : \mathcal{S} \to \mathbb{R}^{p_2}$ & Interaction feature mapping & Maps context to interaction features, which are multiplied by $\mathcal{A}$ \\
$\Theta$ & `True' generative model coefficients & From regression on HSv1 data:\newline $\mathcal{R} \sim [f_1(\mathcal{S}), f_2(\mathcal{A} \cdot \mathcal{S})]^T\Theta$ \\
$\varepsilon, \epsilon_{n,t,d}$ & Linear model residuals & Residuals from HSv1 data after regression \\
$N$ & Number of users  & $N = 37$ in HSv1; $N = 500$ in simulations \\
$T$ & Number of days in study & $T = 42$  \\
$D$ & Number of decision points per day &  Set to $D = 5$ \\
$K$ & Number of cross-validations per test & Set to $K = 3$ 
\\\bottomrule
\end{tabular*}
  \end{table}



\section{Reward Generative Model}

We set different `true' generative models, where we assume:
\begin{equation}
\label{True Generative Model}
\mathcal{R} = \begin{bmatrix}f_1(\mathcal{S}) \\
\mathcal{A} \odot f_2(\mathcal{S})
\end{bmatrix}^T \Theta + \varepsilon
\end{equation}

and that $f_1, f_2$ are feature functions from our set of features $\mathcal{S}$ to baseline and interaction terms, $\odot$ denotes the term-wise product, and each $\varepsilon \sim \mathcal{N}\left(0, \sigma^2 \right)$ for $\sigma^2$, which is a tuning parameter with estimator $\hat{\sigma^2} = \operatorname{Var}(\varepsilon)$.  

MOVE THIS PART
Recall that the Signal-to-Noise-Ratio (SNR) is computed as 
\begin{equation}
\label{SNR equation}
	\frac{\Var(\mathcal{R})}{\sigma^2} = \frac{\Var\left(\begin{bmatrix}
f_1(\mathcal{S}) \\ A \odot f_2(\mathcal{S})	
\end{bmatrix}^T\Theta\right)}{\sigma^2}.
\end{equation}

We describe these models more in Section \ref{Models/Reward Generative Models}.





\section{Cross-Validation}

We use $K = 3$ fold cross-validation to separate training and testing batches.  We do not train the bandit algorithm's parameters on the test batches, as to simulate application of HSv1 data for HSv2.  

To do this, we randomly order the $N = 37$ users, then group the randomly ordered users into $K$ groups; the $K$-th fold cross-validation is performed holding the $K$-th group out as the test sample, and the remaining groups in as the training sample.

For each user, we have a series across all days $T$ and decision points $t$ per day of Reward, Action, and Context.  We will refer to them jointly as $(R, A, S)_{(N,T,t)}$, which are implicitly indexed in order by the user, day, and decision point.  Thus, there are $N \times T \times t$ data points.  



\section{Residual Formation}

Within each test batch and train batch, we conduct ordinary least squares linear regression to residualize additional effects, missing from our model due to possible misspecification, for each user from the baseline and interaction effects.  Specifically, we use the model in Equation \ref{True Generative Model}.

Recall that $f_1$, $f_2$ are the baseline and interaction functions, that are parameters of the generative model.

We thus obtain a `true' $\Theta$ for the simulation as well as a series of $\varepsilon$ for each data point.


\section{Simulated User Generation}

For both the training and test original users' series of $(\mathcal{R}, \mathcal{A}, \mathcal{S})$, we can create train and test batches of $N = 500$ users each by randomly sampling with replacement from the train and test pools respectively.  Picking a high $N$ for simulations gives statistical significance in our stochastic algorithm.  

We note finally that in each of the $N = 37$ original HSv1 users, we imputed the mean value for missing features, and set availability to True when the reward and at least one of the features is measured.




\section{Training Bandit Algorithm Tuning Parameters}

For each variant of the Bandit Algorithm, we will tune parameters differently.  These will be addressed in Section \ref{Models/Bandit Algorithm Variants}.

Ultimately, the models will be tuned according to minimizing the mean $MUER$, subject to reasonable quality metrics and $MUER$ standard deviation.


\section{Simulated User Testing and Quality Metrics}

Once the tuning parameters have been optimized for the batch, we run the test users on with these parameters, and analyze the quality metrics described below:

\begin{enumerate}
	\item Examine time series of $\pi_t(1 | S_t)$, the probability of taking action $1$ for all $t$, looking through several users.
	\item Examine $\pi_t(1 | S_t)$ vs $opt_t(S_t)$ for all $t$, looking through several users.
	\subitem We define $opt_t(S_t)$ as the optimal probability in context $S_t$:
	\begin{equation}
	\label{opt equation}
		opt_t(S_t) = 0.8\ \mathbb{1}\{\text{optimal action is $1$ in $S_t$}\} + 0.2\ \mathbb{1}\{\text{optimal action is $0$ in $S_t$}\} .
	\end{equation}
	\item Examine $|\pi_t(1 | S_t) - opt_t(S_t)|$ for all users, averaging over all $N$ users for each time point $t$.
	\item Examine $|\pi_t(1 | S_t) - opt_t(S_t)|$ for all users, plotting histogram of each user's mean
	\item Examine cumulative regret over $t$, plotting average over all users as well as for several individual users. 
	\subitem Cumulative regret is the expected reward of the bandit minus reward of the optimal policy.
	\item Examine the number of actions taken at each time $t$, plotting histogram across all $t$ for each of several simulated users, as well as the average taken across all $N$ users plotted at each time $t$.
\end{enumerate}



\begin{algorithm}[H]
\label{Simulated User Algorithm}
 \KwData{$(\mathcal{S},\varepsilon)_{N,T,D}^{batch}$}
 \KwResult{$(\mathcal{S},\varepsilon)_{N,T,D}^{sim}$}
 \For{$1 \le n \le N_{new}$}{
 \While{$(\mathcal{S},\varepsilon)_{n,:,:}^{batch}$ does not have $N_{sim}$ data points} {
  	Sample a single user $n' \in [1,N_{batch}]$ without replacement\;
  	Append $(S,\epsilon)_{n',:,:}^{batch}$ to $(S, \epsilon)_{n,:,:}^{sim}$\;
  }
 }
 \caption{Simulated User Generation Pseudocode}
\end{algorithm}