{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T23:05:39.970822Z",
     "start_time": "2018-03-09T23:05:27.301957Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "#################\n",
    "# Test Bed 2-25 #\n",
    "#################\n",
    "# HS Analysis Functions\n",
    "\n",
    "### Aux Functions for reading two different .csv files ###\n",
    "def read_hs1(yoozer, hs1_file = \"../../../Data/suggest-analysis-kristjan.csv\"):\n",
    "    pd_df = pd.read_csv(hs1_file) #'suggest-analysis-kristjan.csv')\n",
    "    pdf = pd_df[:179]\n",
    "    snda = pd_df['send.active'] == 1\n",
    "    snd = pd_df['send'] == 0\n",
    "    usr = pd_df['user'] == yoozer\n",
    "    ddf = pd_df[(snda | snd) & usr]\n",
    "    ddf = ddf.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    return ddf,pd_df\n",
    "\n",
    "def read_hs1_gf(yoozer, hs1_gf_file = '../../../Data/suggest-kristjan.csv'):\n",
    "    pd_df = pd.read_csv(hs1_gf_file) #'suggest-analysis-kristjan.csv')\n",
    "    pdf = pd_df[:179]\n",
    "    snda = pd_df['send.active'] == 1\n",
    "    snd = pd_df['send'] == 0\n",
    "    usr = pd_df['user'] == yoozer\n",
    "    ddf = pd_df[(snda | snd) & usr]\n",
    "    ddf = ddf.reset_index(drop=True)\n",
    "\n",
    "    return ddf,pd_df\n",
    "\n",
    "\n",
    "def nan_equal(a,b):\n",
    "    return ((a == b) | (np.isnan(a) & np.isnan(b))).all()\n",
    "\n",
    "\n",
    "def standardize_column(ddf, col_name):\n",
    "    '''Standardizes column col_name in ddf inplace, converting to a float too'''\n",
    "    ddf.loc[:,col_name] = ddf[col_name].astype(float)\n",
    "    ddf.loc[:,col_name] = (ddf[col_name] - ddf[col_name].mean()) / ddf[col_name].std()\n",
    "\n",
    "\n",
    "def read_data_slow(N, T, t, nBaseline, data_loc_pref = \"C:/Users/isaac/Dropbox/Harvard 17-18 Senior/Thesis/Data/\"):\n",
    "    ### Read in Feature vector, Reward Vector, and Action Vector for each user ###\n",
    "    # Also Standardizes all features\n",
    "\n",
    "    max_yoozer = N+1\n",
    "\n",
    "    featVec = np.empty((max_yoozer,T*t, nBaseline))\n",
    "    featVec.fill(np.nan)\n",
    "\n",
    "    rewardVec = np.empty((max_yoozer,T*t))\n",
    "    rewardVec.fill(np.nan)\n",
    "\n",
    "    actionVec = np.empty((max_yoozer,T*t))\n",
    "    actionVec.fill(np.nan)\n",
    "\n",
    "    for yoozer in range(1,max_yoozer):\n",
    "\n",
    "        ddf,pd_df = read_hs1(yoozer, data_loc_pref + \"suggest-analysis-kristjan.csv\")\n",
    "        #Make features\n",
    "        \n",
    "        #Center and scale\n",
    "        decision_ind = ddf['decision.index.nogap']\n",
    "        reward_h = ddf['jbsteps30.log']\n",
    "        send_any = ddf['send']\n",
    "        send_active = ddf['send.active']\n",
    "\n",
    "        assert np.all(send_any == send_active)\n",
    "\n",
    "        # Study day index\n",
    "        dazze = ddf['study.day.nogap']\n",
    "\n",
    "        # Number of messages sent in last week\n",
    "        day_ind  = ddf['study.day.nogap'].astype(float)\n",
    "        day_ind = (day_ind - np.mean(day_ind))/np.std(day_ind)\n",
    "\n",
    "        # Ohter location indicator\n",
    "        loc_ind = ddf['loc.is.other'].astype(float)\n",
    "        loc_ind = (loc_ind - np.mean(loc_ind))/np.std(loc_ind)\n",
    "\n",
    "        # Std deviation of step count in last 7 days\n",
    "        steps_sd = ddf['window7.steps60.sd'].astype(float)\n",
    "        steps_sd = (steps_sd - np.mean(steps_sd))/np.std(steps_sd)\n",
    "\n",
    "        # Step count in previous 30 minutes\n",
    "        state = ddf['jbsteps30pre.log'].astype(float)\n",
    "        state = (state - np.mean(state))/np.std(state)\n",
    "        \n",
    "        # Work location indicator\n",
    "        wrk_ind = ddf['loc.is.work'].astype(float) #compare to string “work”\n",
    "        wrk_ind = (wrk_ind - np.mean(wrk_ind))/np.std(wrk_ind)\n",
    "\n",
    "        # Square root steps yesterday\n",
    "        steps_yest = ddf['steps.yesterday.sqrt'].astype(float)\n",
    "        steps_yest = (steps_yest - np.mean(steps_yest))/np.std(steps_yest)\n",
    "\n",
    "        # Temperature, set -1024 as 0\n",
    "        temp = ddf['temperature'].astype(float)\n",
    "        temp[ddf['temperature'] == -1024] = 0\n",
    "        temp = (temp - np.mean(temp))/np.std(temp)\n",
    "        \n",
    "\n",
    "        ddfgf,pd_dfgf = read_hs1_gf(yoozer, data_loc_pref + \"suggest-kristjan.csv\")\n",
    "\n",
    "\n",
    "        end_ind = day_ind.shape[0]\n",
    "\n",
    "\n",
    "        # Set reward, action, and state\n",
    "        rewardVec[yoozer,:end_ind] = reward_h.astype(float)\n",
    "        actionVec[yoozer,:end_ind] = send_any.astype(float)\n",
    "\n",
    "        featVec[yoozer,:end_ind,0].fill(1) # Only fill rows with observations\n",
    "        featVec[yoozer,:end_ind,1] = day_ind #study.day.nogap\n",
    "        featVec[yoozer,:end_ind,2] = loc_ind # loc.is.other\n",
    "        featVec[yoozer,:end_ind,3] = steps_sd # window7.steps60.sd\n",
    "        featVec[yoozer,:end_ind,4] = state # jbsteps30pre.log\n",
    "        featVec[yoozer,:end_ind,5] = wrk_ind # loc.is.work\n",
    "        featVec[yoozer,:end_ind,6] = steps_yest # steps.yesterday.sqrt\n",
    "        featVec[yoozer,:end_ind,7] = temp # temperature\n",
    "\n",
    "    \n",
    "    ## TREAT DATA ##\n",
    "\n",
    "    # Drop 0th user, since users are 1-indexed\n",
    "    featVec = featVec[1:,:,:].copy()\n",
    "    rewardVec = rewardVec[1:,:].copy()\n",
    "    actionVec = actionVec[1:,:].copy()\n",
    "\n",
    "    # Reshape actionVec to have additional 1 dim\n",
    "    actionVec = actionVec.reshape(actionVec.shape[0],actionVec.shape[1],1)\n",
    "\n",
    "    # Mean impute featVec where there is an observation\n",
    "    featVec[~np.isnan(featVec[:,:,1])] = np.nan_to_num(featVec[~np.isnan(featVec[:,:,1])])\n",
    "    \n",
    "    \n",
    "    # Drop users with no data\n",
    "    rewardVec = rewardVec[~np.isnan(featVec).all(axis=(1,2))]\n",
    "    actionVec = actionVec[~np.isnan(featVec).all(axis=(1,2))]\n",
    "    featVec = featVec[~np.isnan(featVec).all(axis=(1,2))]\n",
    "    \n",
    "    return featVec, rewardVec, actionVec\n",
    "\n",
    "\n",
    "def read_data(N, T, t, nBaseline, data_loc_pref):\n",
    "\n",
    "    ddf = pd.read_csv(data_loc_pref + \"suggest-analysis-kristjan.csv\")\n",
    "    # Filter for when either sent active message or no message at all, and relevant columns\n",
    "    ddf = ddf.loc[(ddf[\"send.active\"] == 1) | (ddf[\"send\"] == 0),[\"user\",\"study.day.nogap\",\"loc.is.other\",\"window7.steps60.sd\",\"jbsteps30pre.log\",\"loc.is.work\",\"steps.yesterday.sqrt\",\"temperature\",\"send\",\"send.active\",\"jbsteps30.log\"]]\n",
    "\n",
    "    # Standardize columns\n",
    "    standardize_column(ddf,\"study.day.nogap\")\n",
    "    standardize_column(ddf,\"loc.is.other\")\n",
    "    standardize_column(ddf,\"window7.steps60.sd\")\n",
    "    standardize_column(ddf,\"jbsteps30pre.log\")\n",
    "    standardize_column(ddf,\"loc.is.work\")\n",
    "    standardize_column(ddf,\"steps.yesterday.sqrt\")\n",
    "    ddf.loc[ddf[\"temperature\"] == -1024,\"temperature\"] = np.nan # replace bad temperature measurement with null\n",
    "    standardize_column(ddf,\"temperature\")\n",
    "    ddf.loc[ddf[\"temperature\"].isnull(),\"temperature\"] = ddf[\"temperature\"].mean()\n",
    "\n",
    "\n",
    "    featVec = np.empty((N+1,T*t, nBaseline))\n",
    "    featVec.fill(np.nan)\n",
    "\n",
    "    rewardVec = np.empty((N+1,T*t))\n",
    "    rewardVec.fill(np.nan)\n",
    "\n",
    "    actionVec = np.empty((N+1,T*t))\n",
    "    actionVec.fill(np.nan)\n",
    "\n",
    "    for user in range(1,N+1):\n",
    "        user_ddf = ddf[ddf[\"user\"] == user]\n",
    "        #Center and scale\n",
    "        reward_h = user_ddf['jbsteps30.log']\n",
    "        send_any = user_ddf['send']\n",
    "        send_active = user_ddf['send.active']\n",
    "\n",
    "        assert np.all(send_any == send_active)\n",
    "\n",
    "        # Study day index\n",
    "        dazze = user_ddf['study.day.nogap']\n",
    "\n",
    "        # Number of messages sent in last week\n",
    "        day_ind  = user_ddf['study.day.nogap']\n",
    "\n",
    "        # Other location indicator\n",
    "        loc_ind = user_ddf['loc.is.other']\n",
    "\n",
    "        # Std deviation of step count in last 7 days\n",
    "        steps_sd = user_ddf['window7.steps60.sd']\n",
    "\n",
    "        # Step count in previous 30 minutes\n",
    "        state = user_ddf['jbsteps30pre.log']\n",
    "\n",
    "        # Work location indicator\n",
    "        wrk_ind = user_ddf['loc.is.work']\n",
    "\n",
    "        # Square root steps yesterday\n",
    "        steps_yest = user_ddf['steps.yesterday.sqrt']\n",
    "\n",
    "        # Temperature, set -1024 as 0\n",
    "        temp = user_ddf['temperature']\n",
    "\n",
    "\n",
    "\n",
    "        end_ind = day_ind.shape[0]\n",
    "\n",
    "\n",
    "        # Set reward, action, and state\n",
    "        rewardVec[user,:end_ind] = reward_h.astype(float)\n",
    "        actionVec[user,:end_ind] = send_any.astype(float)\n",
    "\n",
    "        featVec[user,:end_ind,0].fill(1) # Only fill rows with observations\n",
    "        featVec[user,:end_ind,1] = day_ind #study.day.nogap\n",
    "        featVec[user,:end_ind,2] = loc_ind # loc.is.other\n",
    "        featVec[user,:end_ind,3] = steps_sd # window7.steps60.sd\n",
    "        featVec[user,:end_ind,4] = state # jbsteps30pre.log\n",
    "        featVec[user,:end_ind,5] = wrk_ind # loc.is.work\n",
    "        featVec[user,:end_ind,6] = steps_yest # steps.yesterday.sqrt\n",
    "        featVec[user,:end_ind,7] = temp # temperature\n",
    "\n",
    "    ## TREAT DATA ##\n",
    "\n",
    "    # Drop 0th user, since users are 1-indexed\n",
    "    featVec = featVec[1:,:,:].copy()\n",
    "    rewardVec = rewardVec[1:,:].copy()\n",
    "    actionVec = actionVec[1:,:].copy()\n",
    "\n",
    "    # Reshape actionVec to have additional 1 dim\n",
    "    actionVec = actionVec.reshape(actionVec.shape[0],actionVec.shape[1],1)\n",
    "\n",
    "    # Mean impute featVec where there is an observation\n",
    "    featVec[~np.isnan(featVec[:,:,1])] = np.nan_to_num(featVec[~np.isnan(featVec[:,:,1])])\n",
    "    \n",
    "    \n",
    "    # Drop users with no data\n",
    "    rewardVec = rewardVec[~np.isnan(featVec).all(axis=(1,2))]\n",
    "    actionVec = actionVec[~np.isnan(featVec).all(axis=(1,2))]\n",
    "    featVec = featVec[~np.isnan(featVec).all(axis=(1,2))]\n",
    "    \n",
    "    return featVec, rewardVec, actionVec\n",
    "\n",
    "def resid_regression(N, T, t, nBaseline, nInteract, f_baseline, f_interact, rewardVec, actionVec, featVec):\n",
    "    '''Perform Regression on Pooled R ~ A*S' + S to create Residuals'''\n",
    "    # Actual Dimensions of A and S\n",
    "    a_dim = 1\n",
    "    s_dim = nBaseline\n",
    "\n",
    "    # Copy for notational ease\n",
    "    R = rewardVec.copy()\n",
    "    A = actionVec.copy()\n",
    "    S = featVec.copy()\n",
    "\n",
    "\n",
    "    # Fit OLS r ~ (a_t s'_t, s_t)^T \\eta, where s' are interact terms and s are all baseline\n",
    "    \n",
    "    exog = np.concatenate([(A * f_interact(S)), f_baseline(S)], 2).reshape((N*T*t,(nInteract + nBaseline)))\n",
    "    resid_model = sm.OLS(endog = R.reshape(N*T*t), exog = exog, missing = \"drop\")\n",
    "    resids_unproc = resid_model.fit().resid\n",
    "\n",
    "    # Parameters\n",
    "    Thetas_fit = resid_model.fit().params\n",
    "\n",
    "\n",
    "    ## Fill resids ##\n",
    "\n",
    "    # Copy shape and location of nans\n",
    "    resids = rewardVec.copy()\n",
    "\n",
    "    curr_ind = 0\n",
    "    for n in range(N):\n",
    "\n",
    "        old_ind = curr_ind\n",
    "        curr_ind += R[n][~np.isnan(R[n])].shape[0]\n",
    "\n",
    "        # Copy in indices\n",
    "        resids[n][:curr_ind-old_ind] = resids_unproc[old_ind:curr_ind]\n",
    "    \n",
    "    return resids, Thetas_fit, resid_model\n",
    "\n",
    "\n",
    "# # Code to Generate Simulated Users USE IF WANT CUSTOM LENGTH TIME\n",
    "def generate_new_users(resids, A, S, N_new, T_new, T, t, users_to_sample = 10):\n",
    "    '''\n",
    "    Generates new random user, with variable time T_new\n",
    "    \n",
    "    Inputs:\n",
    "        resids: Residuals\n",
    "        A: Actions\n",
    "        S: States\n",
    "        N_new: Int of number of new users to generate from sampling\n",
    "        T_new: Int of number of days for each new user\n",
    "    \n",
    "    Returns:\n",
    "        resids_new: Matrix of resids for sampled users\n",
    "        A_new: Matrix of associated actions for sampled users\n",
    "        S_new: Matrix of associated states for sampled users\n",
    "        \n",
    "    *Assumes shapes:\n",
    "      resids: (N, T * t) \n",
    "      A: (N, T * t, )\n",
    "    '''\n",
    "    \n",
    "    # Obtain original dimensions of data from actions A\n",
    "    N = A.shape[0]\n",
    "\n",
    "    # Component dims of A and S \n",
    "    a_dim = A.shape[2]\n",
    "    s_dim = S.shape[2]\n",
    "\n",
    "    # Sample random users from original data\n",
    "    sampled_user_indices = np.empty((N_new, users_to_sample)).astype(int)\n",
    "\n",
    "    # Loop to reset sampling without replacement for each new user\n",
    "    for i in range(N_new):\n",
    "        sampled_user_indices[i] = np.random.choice(N, size = users_to_sample, replace = False)\n",
    "    \n",
    "    # Concatenate resids, A, and S to ensure processing is uniform\n",
    "    concat_data = np.concatenate([np.expand_dims(resids,2),A,S], axis = 2)\n",
    "    sampled_users_untreated = np.take(concat_data, sampled_user_indices, 0).reshape(N_new, users_to_sample * T*t, 1 + a_dim + s_dim)\n",
    "    sampled_users = np.empty((N_new, T_new * t, 1 + a_dim + s_dim))\n",
    "    \n",
    "    for i in range(N_new):\n",
    "        \n",
    "        # Cast to DataFrame to dropna, reset index to shift datapoints up to be consecutive, takes T_new * t datapoints\n",
    "        sampled_users[i] = pd.DataFrame(sampled_users_untreated[i]).apply(lambda col: col.dropna().reset_index().iloc[:,-1], axis = 0).as_matrix()[:T_new * t]\n",
    "        \n",
    "    # Sampled Generated residuals\n",
    "    resids_new = sampled_users[:,:,0].reshape(N_new, T_new, t)\n",
    "    # Sampled actions\n",
    "    A_new = sampled_users[:,:,1:(1+a_dim)].reshape(N_new, T_new, t, a_dim)\n",
    "    # Sampled states\n",
    "    S_new = sampled_users[:,:,(1+a_dim):].reshape(N_new, T_new, t, s_dim)\n",
    "    \n",
    "    return resids_new, A_new, S_new\n",
    "\n",
    "\n",
    "# # Code to Generate Simulated Users\n",
    "def sample_sim_users(resids, A, S, N_new, T, t, seed = None):\n",
    "    '''\n",
    "    Generates new random users by sampling from the population\n",
    "    \n",
    "    Inputs:\n",
    "        resids: Residuals\n",
    "        A: Actions\n",
    "        S: States\n",
    "        N_new: Int of number of users to generate from sampling\n",
    "    \n",
    "    Returns:\n",
    "        resids_new: Matrix of resids for sampled users\n",
    "        A_new: Matrix of associated actions for sampled users\n",
    "        S_new: Matrix of associated states for sampled users\n",
    "        \n",
    "    *Assumes shapes:\n",
    "      resids: (N, T * t) \n",
    "      A: (N, T * t, )\n",
    "    '''\n",
    "    # Obtain original dimensions of data from actions A\n",
    "    N = A.shape[0]\n",
    "\n",
    "    # Component dims of A and S \n",
    "    a_dim = A.shape[2]\n",
    "    s_dim = S.shape[2]\n",
    "\n",
    "    # Sample random users from original data\n",
    "    sampled_user_indices = np.random.RandomState(seed).choice(N, size = N_new, replace = True)\n",
    "\n",
    "    # Concatenate resids, A, and S to ensure processing is uniform\n",
    "    concat_data = np.concatenate([np.expand_dims(resids,2),A,S], axis = 2)\n",
    "    sampled_users = np.take(concat_data, sampled_user_indices, 0).reshape(N_new, T * t, 1 + a_dim + s_dim) # resids_dim + actions_dim + states_dim\n",
    "\n",
    "    # Sampled Generated residuals\n",
    "    resids_new = sampled_users[:,:,0].reshape(N_new, T, t)\n",
    "    # Sampled actions\n",
    "    A_new = sampled_users[:,:,1:(1+a_dim)].reshape(N_new, T, t, a_dim)\n",
    "    # Sampled states\n",
    "    S_new = sampled_users[:,:,(1+a_dim):].reshape(N_new, T, t, s_dim)\n",
    "    \n",
    "    return resids_new, A_new, S_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create a reward function from true coefficients, with residuals non-mandatory\n",
    "def reward_func_identity(S, A, coef0, coef1, resid = None, single_dim = True):\n",
    "    '''Basic reward function, can edit for different generative models.\n",
    "    Identity on baseline and interaction terms.\n",
    "    Works for multidimensional eta, a, and s, so long as they are of same multidimension.\n",
    "    Assumes s has first element as bias for the regression.\n",
    "    \n",
    "    Resid must be passed in if not single dim.\n",
    "    If single dim, can speed out without np.take.'''\n",
    "    \n",
    "    nInteract = len(coef1)\n",
    "    \n",
    "    if single_dim:\n",
    "        predictors = np.concatenate([A * S[1:1+nInteract], S], 0)\n",
    "    else:\n",
    "        predictors = np.concatenate([A * np.take(S,range(1,1+nInteract),-1), S], 0)\n",
    "    \n",
    "    Theta = np.concatenate([coef1, coef0])\n",
    "    \n",
    "    if resid is None:\n",
    "        resid = 0\n",
    "        \n",
    "    return(resid + np.dot(predictors, Theta))\n",
    "\n",
    "\n",
    "# #Create a reward function from true coefficients, with residuals non-mandatory\n",
    "# def reward_func_small(S, A, coef0, coef1, resid = None, single_dim = True):\n",
    "#     '''Basic reward function, can edit for different generative models.\n",
    "#     Identity on baseline and interaction terms.\n",
    "#     Works for multidimensional eta, a, and s, so long as they are of same multidimension.\n",
    "#     Assumes s has first element as bias for the regression.\n",
    "    \n",
    "#     Resid must be passed in if not single dim.\n",
    "#     If single dim, can speed out without np.take.'''\n",
    "    \n",
    "#     nInteract = len(coef1)\n",
    "    \n",
    "#     if single_dim:\n",
    "#         predictors = np.concatenate([A * S[1:1+nInteract], S], 0)\n",
    "#     else:\n",
    "#         predictors = np.concatenate([A * np.take(S,range(1,1+nInteract),-1), S], 0)\n",
    "    \n",
    "#     Theta = np.concatenate([coef1, coef0])\n",
    "    \n",
    "#     if resid is None:\n",
    "#         resid = 0\n",
    "        \n",
    "#     return(resid + np.dot(predictors, Theta))\n",
    "\n",
    "\n",
    "\n",
    "#Create a reward function from true coefficients, with residuals non-mandatory\n",
    "def reward_func_general(S, A, coef0, coef1, f_baseline, f_interact, resid = None):\n",
    "    '''Generalized reward function, can edit for different generative models.\n",
    "    Works for multidimensional eta, a, and s, so long as they are of same multidimension.\n",
    "    Assumes s has first element as bias for the regression.\n",
    "    \n",
    "    Resid must be passed in if not single dim.\n",
    "    If single dim, can speed out without np.take.'''\n",
    "    \n",
    "    \n",
    "    predictors = np.concatenate([A * f_interact(S), f_baseline(S)], 0)\n",
    "    \n",
    "    Theta = np.concatenate([coef1, coef0])\n",
    "    \n",
    "    if resid is None:\n",
    "        resid = 0\n",
    "        \n",
    "    return(resid + np.dot(predictors, Theta))\n",
    "\n",
    "\n",
    "def k_fold_split(S, R, A, k = 5, seed = None):\n",
    "    '''\n",
    "    Split S,R,A into k fold train/test batches of roughly size N/k\n",
    "    seed: random seed\n",
    "    '''\n",
    "    combined = np.copy(np.concatenate([S,np.expand_dims(R,-1),A],axis=-1))\n",
    "\n",
    "    np.random.RandomState(seed).shuffle(combined) # Shuffle all together\n",
    "    N = combined.shape[0]\n",
    "\n",
    "    tests = []\n",
    "    trains = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # For each N/k sized batch, return test and train\n",
    "        tests.append(combined[int(i*(N/k)):int((i+1)*(N/k))])\n",
    "        trains.append(np.concatenate([combined[:int(i*(N/k))], combined[int((i+1)*(N/k)):]]))\n",
    "    \n",
    "    train_zip = []\n",
    "    test_zip = []\n",
    "    \n",
    "    # Separate back to (S,R,A)\n",
    "    for train in trains:\n",
    "        train_zip.append((train[:,:,:S.shape[-1]], train[:,:,S.shape[-1]:1+S.shape[-1]], train[:,:,S.shape[-1]+1:]))\n",
    "    for test in tests:\n",
    "        test_zip.append((test[:,:,:S.shape[-1]], test[:,:,S.shape[-1]:1+S.shape[-1]], test[:,:,S.shape[-1]+1:]))\n",
    "        \n",
    "    return train_zip, test_zip\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "#THE BANDIT CODE (in class form)\n",
    "##### A bandit model, consisting of reward coefficient mean and covariance\n",
    "\n",
    "# From Peng's 2-1 algorithm; action-centered bandit (algorithm 2)\n",
    "\n",
    "\n",
    "class model:\n",
    "    def __init__(self,mu,Sigma):\n",
    "        self.mu = mu\n",
    "        self.Sigma = Sigma\n",
    "        \n",
    "\n",
    "#THE BANDIT CODE (in class form)\n",
    "\n",
    "class ContextBandit:\n",
    "    #Bandit Object\n",
    "    def __init__(self, nInteract, prior_model, gamma, f_baseline, f_interact, fc_params = None, pc_params = None, sigma2 = 1., prior_weight = 1., fc_flag = True, pc_flag = True, ac_flag = True):\n",
    "        '''\n",
    "        pc_params: (pi_min, pi_max) parameters for probability clipping\n",
    "        prior_model: prior Gaussian model with \\mu_\\beta, \\Sigma_\\beta\n",
    "        gamma: gamma for GP Prior\n",
    "        fc_params: feedback controller parameters: (\\lambda_c, N_c, T_c)\n",
    "            T_c: maximum number of decision times to count dosage\n",
    "        f_baseline: baseline feature mapping; f: S \\to R^p_1\n",
    "        f_interact: interaction feature mapping; f_interact: S' \\to R^p_2\n",
    "        sigma2: reward noise estimate (variance)\n",
    "        prior_weight: how much to weigh prior, set to 1 for full weight\n",
    "\n",
    "        fc_flag, pc_flag, ac_flag: Whether bandit should include feedback controller, probability clipping, or action centering; True is include, False is not include\n",
    "        '''\n",
    "        \n",
    "        #initialize input parameters\n",
    "\n",
    "\n",
    "        self.fc_flag = fc_flag\n",
    "        self.pc_flag = pc_flag\n",
    "        self.ac_flag = ac_flag\n",
    "\n",
    "\n",
    "        self.nInteract = nInteract\n",
    "        if pc_flag:\n",
    "            self.pi_min = pc_params[0]\n",
    "            self.pi_max = pc_params[1]\n",
    "        self.gamma = gamma\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        self.f_baseline = f_baseline\n",
    "        self.f_interact = f_interact\n",
    "\n",
    "        if fc_flag:\n",
    "            self.lambda_c = fc_params[0]\n",
    "            self.N_c = fc_params[1]\n",
    "            self.T_c = fc_params[2]\n",
    "\n",
    "\n",
    "        \n",
    "        #Initialize model to prior data model\n",
    "        # Mean = prior_weight * prior_model.mu,\n",
    "        # Cov = I_n + prior_weight * prior_model.Sigma\n",
    "        \n",
    "        self.prior_model = prior_model\n",
    "\n",
    "        # This is mu and Sigma\n",
    "        self.current_model = model(prior_weight*prior_model.mu,\n",
    "            (1-prior_weight)*np.eye(len(prior_model.mu)) + prior_weight*prior_model.Sigma)\n",
    "        # This is mu' and Sigma'\n",
    "        self.daystart_model = model(prior_weight*prior_model.mu,\n",
    "            (1-prior_weight)*np.eye(len(prior_model.mu)) + prior_weight*prior_model.Sigma)\n",
    "        \n",
    "    def process_context(self, I_T, S_T, N_t = None):\n",
    "        '''\n",
    "        Inputs context for a given decision point,\n",
    "        updates the current day's model, and returns action probability alpha\n",
    "\n",
    "        Line 20 of algorithm\n",
    "\n",
    "        I_T: Current availability\n",
    "        S_T: Current state\n",
    "        N_t: Dosage over past T_c decision times; number of messages sent since present minus T_c\n",
    "        Returns: alpha, probability of taking action 1\n",
    "        '''\n",
    "        if I_T == 1:\n",
    "            current_model = self.current_model\n",
    "            # Current Posterior mean and variance for interaction term\n",
    "            mu2 = current_model.mu[:self.nInteract]\n",
    "            Sigma2 = current_model.Sigma[:self.nInteract,:self.nInteract]\n",
    "\n",
    "            # Compute probability of unclipped randomization probability\n",
    "            X_mean = self.f_interact(S_T).T.dot(mu2)\n",
    "\n",
    "            if self.fc_flag: # If using feedback controller, subtract fc term\n",
    "                X_mean = self.f_interact(S_T).T.dot(mu2) - self.lambda_c*(N_t - self.N_c)*(N_t - self.N_c > 0)\n",
    "            X_var = self.f_interact(S_T).dot(Sigma2.dot(self.f_interact(S_T).T))\n",
    "\n",
    "            # Actual probability\n",
    "            pi_t = 1 - norm.cdf(0, X_mean, np.sqrt(X_var))\n",
    "\n",
    "            alpha = pi_t\n",
    "\n",
    "            if self.pc_flag:\n",
    "                if pi_t < self.pi_min:\n",
    "                    alpha = self.pi_min\n",
    "                elif pi_t > self.pi_max:\n",
    "                    alpha = self.pi_max\n",
    "\n",
    "        \n",
    "        else:\n",
    "            alpha = 0\n",
    "\n",
    "        # Update current model to posterior of Gaussian Process\n",
    "        self.current_model.mu = (1-self.gamma)*self.prior_model.mu + self.gamma * self.current_model.mu\n",
    "        self.current_model.Sigma = (1 - self.gamma ** 2.) * self.prior_model.Sigma + (self.gamma ** 2.) * self.current_model.Sigma \n",
    "\n",
    "        return alpha #probability of taking action 1\n",
    "\n",
    "        \n",
    "    ######\n",
    "    \n",
    "    def update_model_daily(self, S_T, R_T, A_T, pi_T, I_T, t = 5):\n",
    "        # Update at end of day\n",
    "        '''\n",
    "        At end of day, updates the \n",
    "        S_T: states for day T\n",
    "        R_T: rewards for day T\n",
    "        A_T: actions for day T\n",
    "        pi_T: probabilities of action 1 for day T\n",
    "        I_T: availability for each time point on day T\n",
    "        t: number of decision points per day, defaults to 5\n",
    "        '''\n",
    "\n",
    "        # Set aliases for the current model\n",
    "        current_model = self.current_model\n",
    "        current_model.mu = self.daystart_model.mu\n",
    "        current_model.Sigma = self.daystart_model.Sigma\n",
    "\n",
    "        for dpt in range(t):\n",
    "            # If available then:\n",
    "            if I_T[dpt]:\n",
    "                if self.ac_flag:\n",
    "                    f_t = np.concatenate([(A_T[dpt] - pi_T[dpt]) * self.f_interact(S_T[dpt]), self.f_baseline(S_T[dpt])], axis=0)\n",
    "                else:\n",
    "                    f_t = np.concatenate([(A_T[dpt]) * self.f_interact(S_T[dpt]), self.f_baseline(S_T[dpt])], axis=0)\n",
    "            \n",
    "                Sigma_dot_f_t = np.expand_dims(current_model.Sigma.dot(f_t),1)\n",
    "                denom = self.sigma2 + f_t.T.dot(Sigma_dot_f_t)\n",
    "                beta = current_model.mu + (R_T[dpt] - f_t.T.dot(current_model.mu)) / denom * Sigma_dot_f_t\n",
    "                Omega = current_model.Sigma - 1. / denom * Sigma_dot_f_t.dot(Sigma_dot_f_t.T)\n",
    "                current_model.mu = self.gamma * beta           + (1 - self.gamma) * self.prior_model.mu\n",
    "                current_model.Sigma = (self.gamma ** 2.) * Omega + (1 - self.gamma ** 2.) * self.prior_model.Sigma\n",
    "        \n",
    "        # New day, new model\n",
    "\n",
    "        self.daystart_model.mu = current_model.mu.copy()\n",
    "        self.daystart_model.Sigma = current_model.Sigma.copy()\n",
    "        \n",
    "########\n",
    "\n",
    "#####################################################################\n",
    "#A simulation of above standard model.\n",
    "\n",
    "#A simulation of above standard model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T23:05:58.914050Z",
     "start_time": "2018-03-09T23:05:58.909037Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def f_baseline_identity(featVec):\n",
    "    return featVec\n",
    "\n",
    "def f_interact_identity(featVec):\n",
    "    return featVec[...,:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T23:11:32.990463Z",
     "start_time": "2018-03-09T23:11:32.803974Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Counts from HS 1\n",
    "N_data = 48 # users indexed up to 48, but true count is 37\n",
    "N = 37\n",
    "T = 42\n",
    "t = 5\n",
    "nFeatures = 1+7\n",
    "nBaseline_identity = 1+7\n",
    "nInteract_identity = 1+3 # Add 1 for bias term\n",
    "nBaseline_small = 1+4\n",
    "nInteract_small = 1+2 # Add 1 for bias term\n",
    "data_loc_pref = \"../../../Data/\"\n",
    "\n",
    "S, R, A = read_data(N_data, T, t, nFeatures, data_loc_pref)\n",
    "\n",
    "small_flag = False\n",
    "\n",
    "N_sim = 50\n",
    "\n",
    "\n",
    "if small_flag:\n",
    "    f_interact = f_interact_small\n",
    "    f_baseline = f_baseline_small\n",
    "    nInteract = nInteract_small\n",
    "    nBaseline = nBaseline_small\n",
    "else:\n",
    "    f_interact = f_interact_identity\n",
    "    f_baseline = f_baseline_identity\n",
    "    nInteract = nInteract_identity\n",
    "    nBaseline = nBaseline_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T23:11:33.365460Z",
     "start_time": "2018-03-09T23:11:33.320344Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "resids, Thetas_fit, resid_model = resid_regression(N, T, t, nBaseline, nInteract, f_baseline_identity, f_interact_identity, R, A, S)\n",
    "resids_new, A_new, S_new = sample_sim_users(resids, A, S, N_sim, T, t)\n",
    "I_new = np.expand_dims(~np.isnan(S_new).any(axis=-1),-1).astype(int) #\n",
    "\n",
    "\n",
    "\n",
    "resid_sig2 = np.nanvar(resids)\n",
    "prior_mean = np.zeros_like(Thetas_fit)\n",
    "prior_cov_mult = 1\n",
    "\n",
    "\n",
    "\n",
    "prior_mdl = model(np.expand_dims(prior_mean,1), np.eye(nInteract+nBaseline) * prior_cov_mult)\n",
    "fc_params = [1, 3,5]\n",
    "gamma = 0.99\n",
    "sig2_mult = 1.\n",
    "sim_seed = 102\n",
    "ac_flag = True\n",
    "fc_flag = True\n",
    "pc_flag = True\n",
    "small_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T23:35:09.901002Z",
     "start_time": "2018-03-09T23:35:09.481910Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_simulation(coef0, coef1, S_sim, I_sim, resids_sim, reward_func, prior_model,f_baseline, f_interact,f_baseline_identity, f_interact_identity,\n",
    "                   fc_params = None, nInteract = 3+1, nBaseline = 7+1, pc_params = [.1, .8],\n",
    "                   gamma = 1., sigma2 = 1. , T = 42, t = 5, no_resid_flag = False,\n",
    "                   seed = None, fc_flag = True, pc_flag = True, ac_flag = True, Thetas_fit_bandit = None):\n",
    "    '''\n",
    "    coef0: Reward coefficients for baseline, first element is bias term\n",
    "    coef1: Reward coefficients for interaction, first element is bias term; note that concatenated, [coef1, coef0] = Theta\n",
    "    S_sim: simulated states\n",
    "    I_sim: simulated availabilities; 1 = available, 0 = unavailable\n",
    "    resids_sim: Residuals of simulated users\n",
    "    reward_func: reward function of \"True\" Generative Model, must have type reward(featVec, action, coef0, coef1, resid)\n",
    "    nInteract: Number of interaction features in Bandit Model\n",
    "    nBaseline: Number of baseline features in Bandit Model\n",
    "    pc_params: Bandit probability clipping parameters; [minprob, maxprob, priorweight]\n",
    "    f_baseline: Creates baseline features from state in Bandit Model\n",
    "    T: days of study\n",
    "    t: decision points per day\n",
    "    no_resid_flag: If True will substitute 0 for residuals, if False will use true residuals\n",
    "\n",
    "    fc_flag, pc_flag, ac_flag: Whether bandit should include feedback controller, probability clipping, or action centering; True is include, False is not include\n",
    "    '''\n",
    "    N = S_sim.shape[0]\n",
    "    assert T == S_sim.shape[1]\n",
    "    assert t == S_sim.shape[2]\n",
    "    \n",
    "    #creating variables for saving history of what we do\n",
    "    # reward_exp = np.empty((N, T, t)) # Bandit reward\n",
    "    regret = np.empty((N, T, t))\n",
    "    prob  = np.empty((N, T, t))\n",
    "    action = np.empty((N, T, t))\n",
    "    opt = np.empty((N, T, t))\n",
    "    fc_invoked = np.empty((N, T, t))\n",
    "    regret.fill(np.nan)\n",
    "    prob.fill(np.nan)\n",
    "    action.fill(np.nan)\n",
    "    opt.fill(np.nan)\n",
    "    fc_invoked.fill(np.nan)\n",
    "    \n",
    "    seed_rand = np.random.RandomState(seed)\n",
    "    \n",
    "    # Containers for bandit parameters\n",
    "    #bandit_covs = np.empty((N, T,t, nInteract+nBaseline, nInteract+nBaseline))\n",
    "    #bandit_covs.fill(np.nan)\n",
    "    theta_mse = np.empty((N, T,t))\n",
    "    theta_mse.fill(np.nan)\n",
    "\n",
    "    for sim_user_index in range(N):\n",
    "\n",
    "        # Create and initialize bandit object for each user\n",
    "        bandit = ContextBandit(nInteract = nInteract, prior_model = prior_model, gamma = gamma, f_baseline = f_baseline, f_interact = f_interact, sigma2 = sigma2, pc_params = pc_params, fc_params = fc_params, fc_flag = fc_flag, pc_flag = pc_flag, ac_flag = ac_flag)\n",
    "\n",
    "\n",
    "        #Cycle through days where not all values are nan\n",
    "        T_max = np.where(np.isnan(resids_sim[sim_user_index].sum(axis=1)))[0][0]\n",
    "        \n",
    "        for day in range(T_max):\n",
    "\n",
    "            rwd = np.zeros(t)\n",
    "            #Cycle thru decision points in each day\n",
    "            for dpt in range(t):\n",
    "                user_resid = resids_sim[sim_user_index, day, dpt]\n",
    "                if no_resid_flag:\n",
    "                    user_resid = 0\n",
    "                    \n",
    "                #Get action probability from bandit\n",
    "                \n",
    "                # featInteract = S_sim[sim_user_index, day, dpt, 1:1+nInteract] #only use the interaction features, but first element is bias\n",
    "                if False: #Do nothing\n",
    "                    prob[sim_user_index, day, dpt] = 0\n",
    "                else: #Use bandit; invoke feedback controller if flag is on\n",
    "                    if fc_flag:\n",
    "                        # Count dosage from previous T_c decision points\n",
    "                        N_t = np.sum(action[sim_user_index].reshape(-1)[max(0,-bandit.T_c + dpt):dpt])\n",
    "                        # Compute clipped probability\n",
    "                        prob[sim_user_index, day, dpt] = bandit.process_context(\n",
    "                        I_sim[sim_user_index, day, dpt], S_sim[sim_user_index, day, dpt], N_t)\n",
    "                        # Save whether feedback controller was used\n",
    "                        fc_invoked[sim_user_index, day, dpt] = (N_t > bandit.N_c)\n",
    "                    else:\n",
    "                        prob[sim_user_index, day, dpt] = bandit.process_context(\n",
    "                        I_sim[sim_user_index, day, dpt], S_sim[sim_user_index, day, dpt])\n",
    "\n",
    "\n",
    "                #Choose action based on probability, i.e. do the randomization\n",
    "                if seed_rand.rand() < prob[sim_user_index, day, dpt]:\n",
    "                    action[sim_user_index, day, dpt] = 1\n",
    "                else:\n",
    "                    action[sim_user_index, day, dpt] = 0\n",
    "\n",
    "                #Receive reward from the universe. \n",
    "                rwd[dpt] = reward_func(S = S_sim[sim_user_index, day, dpt], A = action[sim_user_index, day, dpt], coef0 = coef0, coef1 = coef1, f_baseline = f_baseline_identity, f_interact = f_interact_identity, resid = user_resid)\n",
    "\n",
    "                #What was expected regret given probability chosen by bandit?\n",
    "                #(easy to do since we know the \\theta coefficients, reward under action 1 is reward under action 0 plus \\theta^T s_t, and vice versa)\n",
    "                rwd0 = reward_func(S = S_sim[sim_user_index, day, dpt], A = 0, coef0 = coef0, coef1 = coef1, f_baseline = f_baseline_identity, f_interact = f_interact_identity, resid = user_resid)\n",
    "                rwd1 = reward_func(S = S_sim[sim_user_index, day, dpt], A = 1, coef0 = coef0, coef1 = coef1, f_baseline = f_baseline_identity, f_interact = f_interact_identity, resid = user_resid)\n",
    "\n",
    "                #Expected reward under bandit policy\n",
    "                rwdExp = prob[sim_user_index, day, dpt]*rwd1 + (1-prob[sim_user_index, day, dpt])*rwd0\n",
    "\n",
    "                #Regret is difference between optimal reward and the reward we got\n",
    "                regret[sim_user_index, day, dpt] = max(pc_params[0]*rwd1 + (1-pc_params[0])*rwd0, pc_params[1]*rwd1 + (1-pc_params[1])*rwd0) - rwdExp\n",
    "\n",
    "                opt[sim_user_index, day, dpt] = pc_params[1] * (rwd1 > rwd0) + pc_params[0] * (rwd1 <= rwd0)\n",
    "\n",
    "            #Perform bandit updates at end of day\n",
    "            for dpt in range(t):\n",
    "                #bandit_covs[sim_user_index, day, dpt] = bandit.daystart_model.Sigma\n",
    "                theta_mse[sim_user_index, day, dpt] = ((Thetas_fit_bandit - bandit.daystart_model.mu) ** 2).mean()\n",
    "                bandit.update_model_daily(S_sim[sim_user_index, day], rwd, action[sim_user_index, day], prob[sim_user_index, day], I_sim[sim_user_index, day])\n",
    "\n",
    "    #return reward_exp.reshape((N,T*t)), reward_0.reshape((N,T*t)), reward_1.reshape((N,T*t)), prob.reshape((N,T*t)), action.reshape((N,T*t)), fc_invoked.reshape((N, T*t)), bandit_covs.reshape((N,T*t, nInteract+nBaseline,nInteract+nBaseline)), bandit_means.reshape((N,T*t, nInteract+nBaseline)), bandit\n",
    "    return regret.reshape((N,T*t)), prob.reshape((N,T*t)), action.reshape((N,T*t)), opt.reshape((N, T*t)), fc_invoked.reshape((N, T*t)), theta_mse.reshape((N,T*t)), bandit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T23:35:14.013151Z",
     "start_time": "2018-03-09T23:35:10.944101Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "regret, prob, action, opt, fc_invoked, theta_mse, bandit = run_simulation(coef0 = Thetas_fit[nInteract_identity:nInteract_identity+nBaseline_identity], coef1 = Thetas_fit[0:nInteract_identity], nInteract = nInteract, nBaseline = nBaseline, S_sim = S_new, I_sim = I_new, resids_sim = resids_new, f_baseline=f_baseline, f_interact=f_interact, f_baseline_identity=f_baseline_identity, f_interact_identity=f_interact_identity, reward_func = reward_func_general, fc_params = fc_params, prior_model = prior_mdl, gamma = gamma, sigma2 = resid_sig2*sig2_mult, seed=sim_seed, ac_flag = ac_flag, fc_flag = fc_flag, pc_flag = pc_flag, Thetas_fit_bandit = Thetas_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # reward_exp, reward_0, reward_1, prob, action, fc_invoked, bandit_mean, bandit = run_simulation(coef0 = Thetas_fit[4:12], coef1 = Thetas_fit[0:4], S_sim = S_new, I_sim = I_new, resids_sim = resids_new, f_baseline=f_baseline_identity, f_interact=f_interact_identity, reward_func = reward_func_identity, fc_params = fc_params, prior_model = prior_mdl, gamma = gamma, sigma2 = resid_sig2*sig2_mult, seed=sim_seed)\n",
    "# reward_exp, reward_0, reward_1, prob, action, fc_invoked, bandit = run_simulation(coef0 = Thetas_fit[4:12], coef1 = Thetas_fit[0:4], S_sim = S_new, I_sim = I_new, resids_sim = resids_new, f_baseline=f_baseline_identity, f_interact=f_interact_identity, reward_func = reward_func_identity, fc_params = fc_params, prior_model = prior_mdl, gamma = gamma, sigma2 = resid_sig2*sig2_mult, seed=sim_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T01:10:17.496565Z",
     "start_time": "2018-03-04T01:10:17.451951Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "from itertools import product\n",
    "\n",
    "k = 3\n",
    "\n",
    "overall_seed = 2018\n",
    "overall_seed_state = np.random.RandomState(overall_seed)\n",
    "\n",
    "# batch_nums = [1]\n",
    "# sig2_mults = [0.5,1.,2.]\n",
    "# prior_cov_mults = [0.25,1.,4.]\n",
    "# gammas = [0.6,0.8,0.9,0.99]\n",
    "# lambs = [0.1,1.,10.]\n",
    "# N_c_mults = [0.25,0.5]\n",
    "# T_cs = [5, 10, 20]\n",
    "\n",
    "#DEEP SCAN VALUES\n",
    "batch_nums = list(range(k))\n",
    "sig2_mults = [0.5,1.,2.]\n",
    "prior_cov_mults = [0.1,0.5,1.,2.]\n",
    "gammas = [0.8,0.9,0.99,1.]\n",
    "lambs = [0.1,0.5, 1.,2.]\n",
    "N_c_mults = [0.25, 0.5,0.75]\n",
    "T_cs = [5, 10, 20]\n",
    "\n",
    "sim_count = len(batch_nums)*len(sig2_mults)*len(prior_cov_mults)*len(gammas)*len(lambs)*len(N_c_mults)*len(T_cs)\n",
    "\n",
    "\n",
    "split_seeds = np.array([[split_seed] * int(sim_count / k) for split_seed in overall_seed_state.randint(2**31, size = k)]).flatten()\n",
    "sim_seeds = overall_seed_state.randint(2**31, size = sim_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-04T01:10:17.827913Z",
     "start_time": "2018-03-04T01:10:17.502631Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_num = 0\n",
    "out_dir = \"results/\"\n",
    "data_loc_pref = \"../../../Data/\"\n",
    "param_loc = \"yamls/\"\n",
    "\n",
    "# Load parameters\n",
    "with open(param_loc + \"/params\" + str(sim_num) + \".yaml\") as f:\n",
    "    params = yaml.load(f)\n",
    "\n",
    "locals().update(params)\n",
    "\n",
    "\n",
    "# Generative Model fit\n",
    "\n",
    "def f_baseline_identity(featVec):\n",
    "    return featVec\n",
    "\n",
    "def f_interact_identity(featVec):\n",
    "    return featVec[...,:4]\n",
    "\n",
    "#Create a reward function from true coefficients, with residuals non-mandatory\n",
    "def reward_func_general(S, A, coef0, coef1, f_baseline, f_interact, resid = None):\n",
    "    '''Generalized reward function, can edit for different generative models.\n",
    "    Works for multidimensional eta, a, and s, so long as they are of same multidimension.\n",
    "    Assumes s has first element as bias for the regression.\n",
    "    \n",
    "    Resid must be passed in if not single dim.\n",
    "    If single dim, can speed out without np.take.'''\n",
    "    \n",
    "    \n",
    "    predictors = np.concatenate([A * f_interact(S), f_baseline(S)], 0)\n",
    "    \n",
    "    Theta = np.concatenate([coef1, coef0])\n",
    "    \n",
    "    if resid is None:\n",
    "        resid = 0\n",
    "        \n",
    "    return(resid + np.dot(predictors, Theta))\n",
    "\n",
    "\n",
    "\n",
    "## Creating Simulations\n",
    "\n",
    "\n",
    "\n",
    "# Counts from HS 1\n",
    "N_data = 48 # users indexed up to 48, but true count is 37\n",
    "N = 37\n",
    "T = 42\n",
    "t = 5\n",
    "nBaseline = 1+7\n",
    "nInteract = 1+3 # Add 1 for bias term\n",
    "\n",
    "N_sim = 50\n",
    "T_new = 90\n",
    "\n",
    "# start = datetime.datetime.now()\n",
    "\n",
    "S, R, A = read_data(N_data, T, t, nBaseline, data_loc_pref)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T21:28:23.339231Z",
     "start_time": "2018-03-03T21:28:23.305642Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ###############\n",
    "# ## Non-split ##\n",
    "# ###############\n",
    "# resids, Thetas_fit, resid_model = resid_regression(N, T, t, nBaseline, nInteract, f_baseline_identity, f_interact_identity, R, A, S)\n",
    "# resids_new, A_new, S_new = sample_sim_users(resids, A, S, N_sim, T, t)\n",
    "# I_new = np.expand_dims(~np.isnan(S_new).any(axis=-1),-1).astype(int) #\n",
    "\n",
    "\n",
    "\n",
    "# resid_sig2 = np.nanvar(resids)\n",
    "# prior_mean = Thetas_fit\n",
    "\n",
    "\n",
    "\n",
    "# prior_mdl = model(np.expand_dims(prior_mean,1), np.eye(nInteract+nBaseline) * prior_cov_mult)\n",
    "# fc_params = [lamb, int(N_c_mult*T_c), T_c]\n",
    "\n",
    "\n",
    "# # sim_start = datetime.datetime.now()\n",
    "# # reward_exp, reward_0, reward_1, prob, action, fc_invoked, bandit_mean, bandit = run_simulation(coef0 = Thetas_fit[4:12], coef1 = Thetas_fit[0:4], S_sim = S_new, I_sim = I_new, resids_sim = resids_new, f_baseline=f_baseline_identity, f_interact=f_interact_identity, reward_func = reward_func_identity, fc_params = fc_params, prior_model = prior_mdl, gamma = gamma, sigma2 = resid_sig2*sig2_mult, seed=sim_seed)\n",
    "# reward_exp, reward_0, reward_1, prob, action, fc_invoked, bandit = run_simulation(coef0 = Thetas_fit[4:12], coef1 = Thetas_fit[0:4], S_sim = S_new, I_sim = I_new, resids_sim = resids_new, f_baseline=f_baseline_identity, f_interact=f_interact_identity, reward_func = reward_func_identity, fc_params = fc_params, prior_model = prior_mdl, gamma = gamma, sigma2 = resid_sig2*sig2_mult, seed=sim_seed)\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "## Split train and test batches ##\n",
    "##################################\n",
    "train_zip, test_zip = k_fold_split(S,R,A,k,seed = split_seed) ## Seed for split\n",
    "\n",
    "S_train,R_train,A_train = train_zip[batch_num]\n",
    "\n",
    "\n",
    "resids_train, Thetas_fit_train, resid_model_train = resid_regression(S_train.shape[0], T, t, nBaseline, nInteract, f_baseline_identity, f_interact_identity, R_train.squeeze(), A_train, S_train)\n",
    "resids_new_train, A_new_train, S_new_train = sample_sim_users(resids_train, A_train, S_train, N_sim, T, t)\n",
    "I_new_train = np.expand_dims(~np.isnan(S_new_train).any(axis=-1),-1).astype(int) #\n",
    "\n",
    "if train:\n",
    "    resid_sig2 = np.nanvar(resids_train)\n",
    "    prior_mean = np.zeros_like(Thetas_fit_train)\n",
    "\n",
    "\n",
    "prior_mdl = model(np.expand_dims(prior_mean,1), np.eye(nInteract+nBaseline) * prior_cov_mult)\n",
    "fc_params = [lamb, int(N_c_mult*T_c), T_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T21:28:42.361787Z",
     "start_time": "2018-03-03T21:28:39.496443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reward_exp, prob, action, fc_invoked, bandit = run_simulation(coef0 = Thetas_fit_train[4:12], coef1 = Thetas_fit_train[0:4], S_sim = S_new_train, I_sim = I_new_train, resids_sim = resids_new_train, f_baseline=f_baseline_identity, f_interact=f_interact_identity, reward_func = reward_func_identity, fc_params = fc_params, prior_model = prior_mdl, gamma = gamma, sigma2 = resid_sig2*sig2_mult, seed=sim_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T21:31:48.368889Z",
     "start_time": "2018-03-03T21:31:45.534670Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "reward_exp2, prob2, action2, fc_invoked2, bandit = run_simulation(coef0 = Thetas_fit_train[4:12], coef1 = Thetas_fit_train[0:4], S_sim = S_new_train, I_sim = I_new_train, resids_sim = resids_new_train, f_baseline=f_baseline_identity, f_interact=f_interact_identity, reward_func = reward_func_identity, fc_params = fc_params, prior_model = prior_mdl, gamma = gamma, sigma2 = resid_sig2*sig2_mult, seed=sim_seed, pc_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T21:49:14.655619Z",
     "start_time": "2018-03-03T21:49:14.649072Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T21:50:14.217209Z",
     "start_time": "2018-03-03T21:50:14.211697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'111results'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-03T21:36:07.784350Z",
     "start_time": "2018-03-03T21:36:07.777299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, True, True),\n",
       " (True, True, False),\n",
       " (True, False, True),\n",
       " (True, False, False),\n",
       " (False, True, True),\n",
       " (False, True, False),\n",
       " (False, False, True),\n",
       " (False, False, False)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in product([True, False],[True, False],[True, False])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# sim_start = datetime.datetime.now()\n",
    "# reward_exp, reward_0, reward_1, prob, action, fc_invoked, bandit_mean, bandit = run_simulation(coef0 = Thetas_fit_train[4:12], coef1 = Thetas_fit_train[0:4], S_sim = S_new_train, I_sim = I_new_train, resids_sim = resids_new_train, f_baseline=f_baseline_identity, f_interact=f_interact_identity, reward_func = reward_func_identity, fc_params = fc_params, prior_model = prior_mdl, gamma = gamma, sigma2 = resid_sig2*sig2_mult, seed=sim_seed)\n",
    "\n",
    "reward_exp, prob, action, fc_invoked, bandit = run_simulation(coef0 = Thetas_fit_train[4:12], coef1 = Thetas_fit_train[0:4], S_sim = S_new_train, I_sim = I_new_train, resids_sim = resids_new_train, f_baseline=f_baseline_identity, f_interact=f_interact_identity, reward_func = reward_func_identity, fc_params = fc_params, prior_model = prior_mdl, gamma = gamma, sigma2 = resid_sig2*sig2_mult, seed=sim_seed)\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "## Output Data ##\n",
    "#################\n",
    "\n",
    "# datNames = [\"reward_exp\",\"reward_0\",\"reward_1\",\"prob\",\"action\",\"fc_invoked\",\"bandit_mean\"]\n",
    "# dats = [reward_exp,reward_0,reward_1,prob,action,fc_invoked,bandit_mean]\n",
    "\n",
    "# Without bandit_mean\n",
    "datNames = [\"reward_exp\",\"prob\",\"action\",\"fc_invoked\"]\n",
    "dats = [reward_exp,prob,action,fc_invoked]\n",
    "\n",
    "for dat, datName in zip(dats, datNames):\n",
    "    np.save(out_dir + datName + \"_simNum\" + str(sim_num) + \".npy\", dat)\n",
    "\n",
    "\n",
    "#################\n",
    "## Save Params ##\n",
    "#################\n",
    "if train:\n",
    "    params[\"computed\"] = True\n",
    "    params[\"prior_mean\"] = prior_mean.tolist()\n",
    "    params[\"resid_sig2\"] = float(resid_sig2)\n",
    "    params[\"train\"] = False\n",
    "    with open(param_loc + \"/params\" + str(sim_num) + \".yaml\", \"w\") as f:\n",
    "        yaml.dump(params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
